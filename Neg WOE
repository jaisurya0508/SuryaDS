import pandas as pd
import numpy as np
# Step 1: Filter where MATCH_FLAG == 1
filtered_df = df[df['MATCH_FLAG'] == 1]

# Step 2: Filter further where ACC_TYPE is in ['BA', 'ba'] and count unique IDs
result = (
    filtered_df[filtered_df['ACC_TYPE'].isin(['BA', 'ba'])]  # Filter for ACC_TYPE
    .groupby('ACC_TYPE')['UNIQUEID'].nunique()  # Count unique UNIQUEID values
    .reset_index(name='Unique_ID_Count')  # Reset index for a clean output
)



grouped = (
    df[df['Match_flag'] == 1]  # Filter for Match_flag == 1
    .groupby(["App_id", "Acc_type"])  # Group by App_id and Acc_type
    .size()  # Count occurrences
    .reset_index(name="Count")  # Reset index and name the count column
)


# Step 1: Calculate total good (label 0) and bad (label 1) counts
total_good = data['Label_0_Count'].sum()
total_bad = data['Label_1_Count'].sum()

# Step 2: Calculate percentage of good and bad for each category
data['pct_good'] = data['Label_0_Count'] / total_good
data['pct_bad'] = data['Label_1_Count'] / total_bad

# Step 3: Calculate WOE (add a small value to avoid division by zero)
data['WOE'] = np.log((data['pct_good'] + 1e-10) / (data['pct_bad'] + 1e-10))

# Display the result
print(data)




Experian: s3://bmf-analytics-dev-eu-west-2-leobrix/DataDictionaries/ExperianDD.xlsm
TU: s3://bmf-analytics-dev-eu-west-2-leobrix/DataDictionaries/TransUnionRetroDD.pdf




 s3://bmf-analytics-dev-eu-west-2-leobrix/data/Retros/September2024Retros/Experian/RawData/ExperianTradelineData_1k.csv
s3://bmf-analytics-dev-eu-west-2-leobrix/data/Retros/September2024Retros/TransUnion/RawData/TUTradelineData_1k.csv




•	Experian: s3://bmf-analytics-dev-eu-west-2-leobrix/data/Retros/September2024Retros/Experian/RawData/ExperianTradelineData.csv
•	TransUnion: s3://bmf-analytics-dev-eu-west-2-leobrix/data/Retros/September2024Retros/TransUnion/RawData/TUTradelineData.csv
