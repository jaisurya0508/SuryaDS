# 1. **Logistic Regression Model**
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

log_reg = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')
log_reg.fit(X_train, y_train)

# 2. **Predictions (without modifying X_train)**
X_train_pred = X_train.copy()
X_test_pred = X_test.copy()

X_train_pred['y_actual'] = y_train
X_train_pred['y_pred'] = log_reg.predict_proba(X_train)[:, 1]

X_test_pred['y_actual'] = y_test
X_test_pred['y_pred'] = log_reg.predict_proba(X_test)[:, 1]

# 3. **Feature Importance**
coefficients = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': log_reg.coef_[0]})
coefficients['Importance'] = np.abs(coefficients['Coefficient'])
coefficients = coefficients.sort_values(by='Importance', ascending=False)

# 4. **Plot Feature Importance**
plt.figure(figsize=(10, 6))
plt.barh(coefficients['Feature'], coefficients['Importance'], color='lightgreen')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.title('Feature Importance (Logistic Regression)')
plt.gca().invert_yaxis()
plt.show()

# 5. **Run Decile Analysis for Logistic Regression**
log_reg_train_summary, log_reg_train_auc, log_reg_train_gini, log_reg_train_ks = decile_analysis(X_train_pred, 'y_pred', 'y_actual')
log_reg_test_summary, log_reg_test_auc, log_reg_test_gini, log_reg_test_ks = decile_analysis(X_test_pred, 'y_pred', 'y_actual')

# 6. **Print Results for Logistic Regression**
print(f"Logistic Regression - Train AUC: {log_reg_train_auc:.2f}, Gini: {log_reg_train_gini:.2f}, KS: {log_reg_train_ks:.2f}")
print(f"Logistic Regression - Test AUC: {log_reg_test_auc:.2f}, Gini: {log_reg_test_gini:.2f}, KS: {log_reg_test_ks:.2f}")



# 1. **Random Forest Model**
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(random_state=42, class_weight='balanced')
rf_model.fit(X_train, y_train)

# 2. **Predictions (without modifying X_train)**
X_train_pred = X_train.copy()
X_test_pred = X_test.copy()

X_train_pred['y_actual'] = y_train
X_train_pred['y_pred'] = rf_model.predict_proba(X_train)[:, 1]

X_test_pred['y_actual'] = y_test
X_test_pred['y_pred'] = rf_model.predict_proba(X_test)[:, 1]

# 3. **Feature Importance**
importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_model.feature_importances_
})
importance = importance.sort_values(by='Importance', ascending=False)

# 4. **Plot Feature Importance**
plt.figure(figsize=(10, 6))
plt.barh(importance['Feature'], importance['Importance'], color='lightcoral')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance (Random Forest)')
plt.gca().invert_yaxis()
plt.show()

# 5. **Run Decile Analysis for Random Forest**
rf_train_summary, rf_train_auc, rf_train_gini, rf_train_ks = decile_analysis(X_train_pred, 'y_pred', 'y_actual')
rf_test_summary, rf_test_auc, rf_test_gini, rf_test_ks = decile_analysis(X_test_pred, 'y_pred', 'y_actual')

# 6. **Print Results for Random Forest**
print(f"Random Forest - Train AUC: {rf_train_auc:.2f}, Gini: {rf_train_gini:.2f}, KS: {rf_train_ks:.2f}")
print(f"Random Forest - Test AUC: {rf_test_auc:.2f}, Gini: {rf_test_gini:.2f}, KS: {rf_test_ks:.2f}")


# 1. **XGBoost Model**
import xgboost as xgb

dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

params = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'learning_rate': 0.01,
    'max_depth': 2,
    'subsample': 0.3,
    'colsample_bytree': 0.3,
    'lambda': 5.5,
    'alpha': 4.8,
    'gamma': 4.5,
    'scale_pos_weight': 3.2,
    'random_state': 42
}
evals = [(dtrain, 'train'), (dtest, 'eval')]

xgb_model = xgb.train(
    params,
    dtrain,
    num_boost_round=2000,
    early_stopping_rounds=50,
    evals=evals,
    verbose_eval=10
)

# 2. **Predictions (without modifying X_train)**
X_train_pred = X_train.copy()
X_test_pred = X_test.copy()

X_train_pred['y_actual'] = y_train
X_train_pred['y_pred'] = xgb_model.predict(dtrain)

X_test_pred['y_actual'] = y_test
X_test_pred['y_pred'] = xgb_model.predict(dtest)

# 3. **Feature Importance**
importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': xgb_model.get_score(importance_type='weight').values()
})
importance = importance.sort_values(by='Importance', ascending=False)

# 4. **Plot Feature Importance**
plt.figure(figsize=(10, 6))
plt.barh(importance['Feature'], importance['Importance'], color='lightblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance (XGBoost)')
plt.gca().invert_yaxis()
plt.show()

# 5. **Run Decile Analysis for XGBoost**
xgboost_train_summary, xgboost_train_auc, xgboost_train_gini, xgboost_train_ks = decile_analysis(X_train_pred, 'y_pred', 'y_actual')
xgboost_test_summary, xgboost_test_auc, xgboost_test_gini, xgboost_test_ks = decile_analysis(X_test_pred, 'y_pred', 'y_actual')

# 6. **Print Results for XGBoost**
print(f"XGBoost - Train AUC: {xgboost_train_auc:.2f}, Gini: {xgboost_train_gini:.2f}, KS: {xgboost_train_ks:.2f}")
print(f"XGBoost - Test AUC: {xgboost_test_auc:.2f}, Gini: {xgboost_test_gini:.2f}, KS: {xgboost_test_ks:.2f}")










import statsmodels.api as sm
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# 1. **Add Constant for Statsmodels**
# Statsmodels requires explicitly adding a constant for the intercept
X_train_sm = sm.add_constant(X_train)
X_test_sm = sm.add_constant(X_test)

# 2. **Logistic Regression Model**
logit_model = sm.Logit(y_train, X_train_sm)
logit_results = logit_model.fit()

# 3. **Print Model Summary**
print(logit_results.summary())

# 4. **Predictions (without modifying X_train)**
X_train_pred = X_train.copy()
X_test_pred = X_test.copy()

X_train_pred['y_actual'] = y_train
X_train_pred['y_pred'] = logit_results.predict(X_train_sm)

X_test_pred['y_actual'] = y_test
X_test_pred['y_pred'] = logit_results.predict(X_test_sm)

# 5. **Feature Importance**
coefficients = pd.DataFrame({
    'Feature': ['Intercept'] + list(X_train.columns),
    'Coefficient': logit_results.params.values
})
coefficients['Importance'] = np.abs(coefficients['Coefficient'])
coefficients = coefficients.sort_values(by='Importance', ascending=False)

# 6. **Plot Feature Importance**
plt.figure(figsize=(10, 6))
plt.barh(coefficients['Feature'], coefficients['Importance'], color='lightblue')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.title('Feature Importance (Statsmodels Logistic Regression)')
plt.gca().invert_yaxis()
plt.show()

# 7. **Run Decile Analysis for Statsmodels Logistic Regression**
logit_train_summary, logit_train_auc, logit_train_gini, logit_train_ks = decile_analysis(X_train_pred, 'y_pred', 'y_actual')
logit_test_summary, logit_test_auc, logit_test_gini, logit_test_ks = decile_analysis(X_test_pred, 'y_pred', 'y_actual')

# 8. **Print Results for Statsmodels Logistic Regression**
print(f"Statsmodels Logistic Regression - Train AUC: {logit_train_auc:.2f}, Gini: {logit_train_gini:.2f}, KS: {logit_train_ks:.2f}")
print(f"Statsmodels Logistic Regression - Test AUC: {logit_test_auc:.2f}, Gini: {logit_test_gini:.2f}, KS: {logit_test_ks:.2f}")






# Extract p-values from the model
p_values = logit_results.pvalues

# Combine features with p-values into a DataFrame
significant_features = pd.DataFrame({
    'Feature': ['Intercept'] + list(X_train.columns),
    'P-Value': p_values.values
})

# Filter for significant features (p-value < 0.05)
significant_features = significant_features[significant_features['P-Value'] < 0.05]

print("Significant Features Based on P-Values:")
print(significant_features)
