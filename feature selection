import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

# Assuming 'dff' is your dataset
# Train-Test split condition
dff["dataset"] = np.where(dff["UNIQUE_ID"] % 60 < 42, "TRAIN", "TEST")
Train = dff.loc[dff["dataset"] == "TRAIN"]
Test = dff.loc[dff["dataset"] == "TEST"]

# Step 1: Prepare the features and target variable for training
x_train = Train.drop(columns=['bad03_24m', 'dataset', 'UNIQUE_ID'])  # Remove target and unwanted columns
y_train = Train['bad03_24m']
x_test = Test.drop(columns=['bad03_24m', 'dataset', 'UNIQUE_ID'])  # Remove target and unwanted columns
y_test = Test['bad03_24m']

# Print the shapes of the datasets
print(f"Train set shape: {x_train.shape}")
print(f"Test set shape: {x_test.shape}")

# Step 2: Handle missing values by imputing them with -9999
imputer = SimpleImputer(strategy='constant', fill_value=-9999)
x_train_imputed = pd.DataFrame(imputer.fit_transform(x_train), columns=x_train.columns)
x_test_imputed = pd.DataFrame(imputer.transform(x_test), columns=x_test.columns)

# Step 3: Standardize the data (important for many models like logistic regression)
scaler = StandardScaler()
x_train_scaled = pd.DataFrame(scaler.fit_transform(x_train_imputed), columns=x_train_imputed.columns)
x_test_scaled = pd.DataFrame(scaler.transform(x_test_imputed), columns=x_test_imputed.columns)

# Step 4: Initialize the Logistic Regression model
model = LogisticRegression(max_iter=1000, solver='liblinear')

# Forward Selection using Sequential Feature Selector
forward_selector = SequentialFeatureSelector(
    estimator=model,
    n_features_to_select="auto",  # Automatically determine the optimal number of features
    direction='forward',  # Forward selection
    scoring='roc_auc',  # Use AUC as the evaluation metric
    cv=5,  # 5-fold cross-validation
    n_jobs=-1  # Utilize all available processors
)

# Fit the forward selector
forward_selector.fit(x_train_scaled, y_train)

# Get selected features using forward selection
selected_forward_features = x_train_scaled.columns[forward_selector.get_support()]
print("\nSelected features using forward selection:")
print(selected_forward_features)

# Optional: Get cross-validation scores (importance) for selected features
forward_scores = []
for feature in selected_forward_features:
    score = cross_val_score(model, x_train_scaled[[feature]], y_train, cv=5, scoring='roc_auc').mean()
    forward_scores.append((feature, score))

# Create a DataFrame for forward selection scores
forward_scores_df = pd.DataFrame(forward_scores, columns=['Feature', 'AUC_Score'])
print("\nForward selection feature importance (AUC score):")
print(forward_scores_df.sort_values(by='AUC_Score', ascending=False))


# Backward Elimination using Sequential Feature Selector
backward_selector = SequentialFeatureSelector(
    estimator=model,
    n_features_to_select="auto",  # Automatically determine the optimal number of features
    direction='backward',  # Backward elimination
    scoring='roc_auc',  # Use AUC as the evaluation metric
    cv=5,  # 5-fold cross-validation
    n_jobs=-1  # Utilize all available processors
)

# Fit the backward selector
backward_selector.fit(x_train_scaled, y_train)

# Get selected features using backward elimination
selected_backward_features = x_train_scaled.columns[backward_selector.get_support()]
print("\nSelected features using backward elimination:")
print(selected_backward_features)

# Optional: Get cross-validation scores (importance) for selected features
backward_scores = []
for feature in selected_backward_features:
    score = cross_val_score(model, x_train_scaled[[feature]], y_train, cv=5, scoring='roc_auc').mean()
    backward_scores.append((feature, score))

# Create a DataFrame for backward elimination scores
backward_scores_df = pd.DataFrame(backward_scores, columns=['Feature', 'AUC_Score'])
print("\nBackward elimination feature importance (AUC score):")
print(backward_scores_df.sort_values(by='AUC_Score', ascending=False))
