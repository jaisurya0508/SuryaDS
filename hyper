import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# 1. Fit Base Logistic Regression Model
log_reg = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')
log_reg.fit(x_train, y_train)

# Predictions
x_train_pred = x_train.copy()
x_test_pred = x_test.copy()

x_train_pred['y_actual'] = y_train
x_train_pred['y_pred'] = log_reg.predict_proba(x_train)[:, 1]

x_test_pred['y_actual'] = y_test
x_test_pred['y_pred'] = log_reg.predict_proba(x_test)[:, 1]

# 2. **Grid Search for Hyperparameter Tuning**
param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet'],
    'C': [0.01, 0.1, 1, 10, 100],
    'solver': ['liblinear', 'saga'],  # Ensure compatibility with penalty
    'class_weight': ['balanced', None],
    'max_iter': [500, 1000, 1500]
}

grid_search = GridSearchCV(
    estimator=LogisticRegression(random_state=42),
    param_grid=param_grid,
    scoring='roc_auc',
    cv=5,
    verbose=1,
    n_jobs=-1
)
grid_search.fit(x_train, y_train)

# Best Parameters from Grid Search
best_model_grid = grid_search.best_estimator_
x_test_pred['y_pred_grid'] = best_model_grid.predict_proba(x_test)[:, 1]

# 3. **Random Search for Hyperparameter Tuning**
param_dist = {
    'penalty': ['l1', 'l2', 'elasticnet'],
    'C': np.logspace(-3, 3, 50),
    'solver': ['liblinear', 'saga'],
    'class_weight': ['balanced', None],
    'max_iter': [500, 1000, 1500]
}

random_search = RandomizedSearchCV(
    estimator=LogisticRegression(random_state=42),
    param_distributions=param_dist,
    n_iter=50,
    scoring='roc_auc',
    cv=5,
    verbose=1,
    n_jobs=-1,
    random_state=42
)
random_search.fit(x_train, y_train)

# Best Parameters from Random Search
best_model_random = random_search.best_estimator_
x_test_pred['y_pred_random'] = best_model_random.predict_proba(x_test)[:, 1]

# 4. **Decile Analysis**
# Run decile analysis for all models
log_reg_train_summary, log_reg_train_auc, log_reg_train_gini, log_reg_train_ks = decile_analysis(
    x_train_pred, 'y_pred', 'y_actual'
)
log_reg_test_summary, log_reg_test_auc, log_reg_test_gini, log_reg_test_ks = decile_analysis(
    x_test_pred, 'y_pred', 'y_actual'
)

grid_train_summary, grid_train_auc, grid_train_gini, grid_train_ks = decile_analysis(
    x_train_pred, 'y_pred', 'y_actual'
)
grid_test_summary, grid_test_auc, grid_test_gini, grid_test_ks = decile_analysis(
    x_test_pred, 'y_pred_grid', 'y_actual'
)

random_train_summary, random_train_auc, random_train_gini, random_train_ks = decile_analysis(
    x_train_pred, 'y_pred', 'y_actual'
)
random_test_summary, random_test_auc, random_test_gini, random_test_ks = decile_analysis(
    x_test_pred, 'y_pred_random', 'y_actual'
)

# 5. **Print Results**
print(f"Logistic Regression - Train AUC: {log_reg_train_auc:.2f}, Gini: {log_reg_train_gini:.2f}, KS: {log_reg_train_ks:.2f}")
print(f"Logistic Regression - Test AUC: {log_reg_test_auc:.2f}, Gini: {log_reg_test_gini:.2f}, KS: {log_reg_test_ks:.2f}")

print(f"Grid Search - Train AUC: {grid_train_auc:.2f}, Gini: {grid_train_gini:.2f}, KS: {grid_train_ks:.2f}")
print(f"Grid Search - Test AUC: {grid_test_auc:.2f}, Gini: {grid_test_gini:.2f}, KS: {grid_test_ks:.2f}")

print(f"Random Search - Train AUC: {random_train_auc:.2f}, Gini: {random_train_gini:.2f}, KS: {random_train_ks:.2f}")
print(f"Random Search - Test AUC: {random_test_auc:.2f}, Gini: {random_test_gini:.2f}, KS: {random_test_ks:.2f}")

# 6. **Feature Importance Visualization**
best_model = grid_search.best_estimator_  # Choose the best model (Grid Search in this case)
coefficients = pd.DataFrame({
    'Feature': x_train.columns,
    'Coefficient': best_model.coef_[0]
})
coefficients['Importance'] = np.abs(coefficients['Coefficient'])
coefficients = coefficients.sort_values(by='Importance', ascending=False)

# Plot Feature Importance
plt.figure(figsize=(10, 6))
plt.barh(coefficients['Feature'], coefficients['Importance'], color='lightgreen')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.title('Feature Importance (Logistic Regression)')
plt.gca().invert_yaxis()
plt.show()
