import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Assuming 'dff' is your dataset

# Step 1: Select numerical columns only
numerical_data = dff.select_dtypes(exclude='object')

# Step 2: Fill missing values with -9999
imputer = SimpleImputer(strategy='constant', fill_value=-9999)
numerical_data_imputed = pd.DataFrame(imputer.fit_transform(numerical_data), columns=numerical_data.columns)

# Step 3: Standardize the numerical data (PCA requires standardized data)
scaler = StandardScaler()
numerical_data_scaled = pd.DataFrame(scaler.fit_transform(numerical_data_imputed), columns=numerical_data.columns)

# Step 4: Apply PCA to retain 95% of the variance
pca = PCA(n_components=0.95)
pca_data = pd.DataFrame(pca.fit_transform(numerical_data_scaled), columns=[f"pca_{i+1}" for i in range(pca.n_components_)])

# Step 5: Add PCA components back to the dataset
dff_pca = dff.copy()
for col in pca_data.columns:
    dff_pca[col] = pca_data[col]

# Step 6: Split the data into train and test based on UNIQUE_ID modulo condition
dff_pca["dataset"] = np.where(dff_pca["UNIQUE_ID"] % 60 < 42, "TRAIN", "TEST")
Train = dff_pca.loc[dff_pca["dataset"] == "TRAIN"]
Test = dff_pca.loc[dff_pca["dataset"] == "TEST"]

# Step 7: Prepare features and target variable for model training
x_train = Train[[col for col in pca_data.columns]]  # Use PCA features only
y_train = Train['bad03_24m']
x_test = Test[[col for col in pca_data.columns]]  # Use PCA features only
y_test = Test['bad03_24m']

# Step 8: Print the shapes of datasets
print(f"Train set shape: {x_train.shape}")
print(f"Test set shape: {x_test.shape}")
print(f"Number of PCA components used: {x_train.shape[1]}")
