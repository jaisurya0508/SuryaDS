import pandas as pd
import statsmodels.api as sm
import numpy as np

# Step 1: Data Splitting
def split_data(dff):
    """
    Split the dataset into train and test based on the condition.
    """
    dff["dataset"] = np.where(dff["UNIQUE_ID"] % 60 < 42, "TRAIN", "TEST")
    train = dff.loc[dff["dataset"] == "TRAIN"]
    test = dff.loc[dff["dataset"] == "TEST"]
    
    x_train = train.drop(columns=['Final_Bad', 'dataset', 'UNIQUE_ID'])
    y_train = train['Final_Bad']
    
    x_test = test.drop(columns=['Final_Bad', 'dataset', 'UNIQUE_ID'])
    y_test = test['Final_Bad']
    
    return x_train, y_train, x_test, y_test

# Step 2: Fill Null Values
def fill_null_values(x_train, x_test):
    """
    Fill NaN values with -9999 for both train and test datasets.
    """
    x_train = x_train.fillna(-9999)
    x_test = x_test.fillna(-9999)
    return x_train, x_test

# Step 3: Forward Selection
def forward_selection(x_train, y_train, threshold_in=0.05):
    """
    Perform forward selection to select features based on AIC.
    """
    initial_features = x_train.columns.tolist()
    best_features = []
    current_score, best_new_score = float('inf'), float('inf')

    while initial_features:
        scores_with_candidates = []
        for feature in initial_features:
            try:
                model = sm.Logit(y_train, sm.add_constant(x_train[best_features + [feature]])).fit(disp=0)
                aic = model.aic
                scores_with_candidates.append((feature, aic))
            except np.linalg.LinAlgError:
                continue
        
        if not scores_with_candidates:
            break
        
        scores_with_candidates.sort(key=lambda x: x[1])
        best_new_feature, best_new_aic = scores_with_candidates[0]
        
        # If AIC improves, add the feature
        if best_new_aic < current_score - threshold_in:
            best_features.append(best_new_feature)
            initial_features.remove(best_new_feature)
            current_score = best_new_aic
        else:
            break

    return best_features

# Step 4: Backward Elimination
def backward_elimination(x_train, y_train, threshold_out=0.05):
    """
    Perform backward elimination to remove features based on AIC.
    """
    features = x_train.columns.tolist()
    current_score = float('inf')

    while len(features) > 0:
        scores_with_candidates = []
        for feature in features:
            try:
                remaining_features = [f for f in features if f != feature]
                model = sm.Logit(y_train, sm.add_constant(x_train[remaining_features])).fit(disp=0)
                aic = model.aic
                scores_with_candidates.append((feature, aic))
            except np.linalg.LinAlgError:
                continue
        
        if not scores_with_candidates:
            break
        
        scores_with_candidates.sort(key=lambda x: x[1])
        worst_feature, worst_aic = scores_with_candidates[0]
        
        # If removing the feature improves AIC, remove it
        if worst_aic < current_score - threshold_out:
            features.remove(worst_feature)
            current_score = worst_aic
        else:
            break

    return features

# Step 5: Main Execution
def main_feature_selection(dff):
    """
    Perform the entire feature selection process.
    """
    # Step 1: Split the data
    x_train, y_train, x_test, y_test = split_data(dff)

    # Step 2: Fill null values
    x_train, x_test = fill_null_values(x_train, x_test)

    # Step 3: Perform Forward Selection
    forward_selected_features = forward_selection(x_train, y_train)
    print("Forward Selected Features:", forward_selected_features)

    # Step 4: Perform Backward Elimination
    backward_selected_features = backward_elimination(x_train, y_train)
    print("Backward Selected Features:", backward_selected_features)

    return forward_selected_features, backward_selected_features

# Assuming `dff` is your dataset
# forward_selected, backward_selected = main_feature_selection(dff)
