import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Assuming 'dff' is your dataset

# Step 1: Select numerical columns only
numerical_data = dff.select_dtypes(exclude='object')

# Step 2: Fill missing values with -9999
imputer = SimpleImputer(strategy='constant', fill_value=-9999)
numerical_data_imputed = pd.DataFrame(imputer.fit_transform(numerical_data), columns=numerical_data.columns)

# Step 3: Standardize the numerical data (PCA requires standardized data)
scaler = StandardScaler()
numerical_data_scaled = pd.DataFrame(scaler.fit_transform(numerical_data_imputed), columns=numerical_data.columns)

# Step 4: Apply PCA to retain 95% of the variance
pca = PCA(n_components=0.95)
pca_data = pd.DataFrame(pca.fit_transform(numerical_data_scaled), columns=[f"pca_{i+1}" for i in range(pca.n_components_)])

# Step 5: Add PCA components back to the dataset
dff_pca = dff.copy()
for col in pca_data.columns:
    dff_pca[col] = pca_data[col]

# Step 6: Split the data into train and test based on UNIQUE_ID modulo condition
dff_pca["dataset"] = np.where(dff_pca["UNIQUE_ID"] % 60 < 42, "TRAIN", "TEST")
Train = dff_pca.loc[dff_pca["dataset"] == "TRAIN"]
Test = dff_pca.loc[dff_pca["dataset"] == "TEST"]

# Step 7: Prepare features and target variable for model training
x_train = Train[[col for col in pca_data.columns]]  # Use PCA features only
y_train = Train['bad03_24m']
x_test = Test[[col for col in pca_data.columns]]  # Use PCA features only
y_test = Test['bad03_24m']

# Step 8: Print the shapes of datasets
print(f"Train set shape: {x_train.shape}")
print(f"Test set shape: {x_test.shape}")
print(f"Number of PCA components used: {x_train.shape[1]}")



# Get PCA loadings (components) as a DataFrame
loadings = pd.DataFrame(
    pca.components_.T,  # Transpose to match features with components
    index=numerical_data_scaled.columns,  # Original feature names
    columns=[f"PCA_{i+1}" for i in range(pca.n_components_)]
)

# Display the loadings for inspection
print("PCA Loadings (feature contributions to components):")
print(loadings)

# Identify the top contributing features for each principal component
for component in loadings.columns:
    print(f"\nTop contributing features to {component}:")
    print(loadings[component].abs().sort_values(ascending=False).head(10))

# Aggregate contributions across all components
feature_importance = loadings.abs().sum(axis=1).sort_values(ascending=False)

print("\nTop features contributing across all principal components:")
print(feature_importance.head(10))
























from sklearn.feature_selection import VarianceThreshold

# Apply variance threshold
selector = VarianceThreshold(threshold=0.01)  # Select features with variance greater than 1%
x_train_reduced = selector.fit_transform(x_train_scaled)
x_test_reduced = selector.transform(x_test_scaled)

print(f"Reduced feature set shape: {x_train_reduced.shape}")















from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Assuming Train and Test data is already available as x_train_scaled, x_test_scaled

# Reduce features using a variance threshold
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)
x_train_reduced = selector.fit_transform(x_train_scaled)
x_test_reduced = selector.transform(x_test_scaled)

# Initialize the logistic regression model with a more efficient solver
model = LogisticRegression(solver='saga', max_iter=1000)

# Forward Selection with smaller number of features selected at once
forward_selector = SequentialFeatureSelector(
    estimator=model,
    n_features_to_select=10,  # Select 10 features at a time
    direction='forward',
    scoring='roc_auc',
    cv=3,  # Reduce the number of folds to save memory
    n_jobs=-1  # Parallelize the computation
)

# Fit the forward selector
forward_selector.fit(x_train_reduced, y_train)

# Get the selected features
selected_forward_features = x_train_scaled.columns[forward_selector.get_support()]
print("\nSelected features using forward selection:")
print(selected_forward_features)


