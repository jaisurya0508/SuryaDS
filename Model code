import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve

# 1. **Logistic Regression Model (Train only on features)**
log_reg = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')
log_reg.fit(x_train, y_train)  # Train the model using only features (no extra columns added)

# 2. **Predictions (Add 'y_pred' and 'y_actual' after model training)**
# Predict probabilities on the same x_train, x_test (without modifying features during training)
x_train['y_pred'] = log_reg.predict_proba(x_train)[:, 1]  # Predict on x_train and assign predicted probabilities
x_train['y_actual'] = y_train  # Add 'y_actual' after predictions, not during training

x_test['y_pred'] = log_reg.predict_proba(x_test)[:, 1]  # Predict on x_test
x_test['y_actual'] = y_test  # Add 'y_actual' after predictions

# 3. **Feature Importance (As before)**
coefficients = pd.DataFrame({'Feature': x_train.columns, 'Coefficient': log_reg.coef_[0]})
coefficients['Importance'] = np.abs(coefficients['Coefficient'])
coefficients = coefficients.sort_values(by='Importance', ascending=False)

# 4. **Plot Feature Importance (As before)**
plt.figure(figsize=(10, 6))
plt.barh(coefficients['Feature'], coefficients['Importance'], color='lightgreen')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.title('Feature Importance (Logistic Regression)')
plt.gca().invert_yaxis()  # Invert y-axis to display the most important features on top
plt.show()

# 5. **Run Decile Analysis (As before)**
train_summary, train_auc, train_gini, train_ks = decile_analysis(x_train, 'y_pred', 'y_actual')
test_summary, test_auc, test_gini, test_ks = decile_analysis(x_test, 'y_pred', 'y_actual')

# 6. **Print Results (As before)**
print(f"Train AUC: {train_auc:.2f}, Gini: {train_gini:.2f}, KS: {train_ks:.2f}")
print(f"Test AUC: {test_auc:.2f}, Gini: {test_gini:.2f}, KS: {test_ks:.2f}")




from sklearn.ensemble import RandomForestClassifier

# 1. **Random Forest Model**
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    random_state=42,
    class_weight='balanced',
    oob_score=True
)
rf_model.fit(X_train, y_train)

# 2. **Predictions**
X_train['y_actual'] = y_train
X_train['y_pred'] = rf_model.predict_proba(X_train)[:, 1]
X_test['y_actual'] = y_test
X_test['y_pred'] = rf_model.predict_proba(X_test)[:, 1]
X_oot['y_actual'] = y_oot
X_oot['y_pred'] = rf_model.predict_proba(X_oot)[:, 1]

# 3. **Feature Importance**
importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_model.feature_importances_
})
importance = importance.sort_values(by='Importance', ascending=False)

# 4. **Plot Feature Importance**
plt.figure(figsize=(10, 6))
plt.barh(importance['Feature'], importance['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance (Random Forest)')
plt.gca().invert_yaxis()
plt.show()

# 5. **Run Decile Analysis**
train_summary, train_auc, train_gini, train_ks = decile_analysis(X_train, 'y_pred', 'y_actual')
test_summary, test_auc, test_gini, test_ks = decile_analysis(X_test, 'y_pred', 'y_actual')
oot_summary, oot_auc, oot_gini, oot_ks = decile_analysis(X_oot, 'y_pred', 'y_actual')

# 6. **Print Results**
print(f"Train AUC: {train_auc:.2f}, Gini: {train_gini:.2f}, KS: {train_ks:.2f}")
print(f"Test AUC: {test_auc:.2f}, Gini: {test_gini:.2f}, KS: {test_ks:.2f}")
print(f"OOT AUC: {oot_auc:.2f}, Gini: {oot_gini:.2f}, KS: {oot_ks:.2f}")





import xgboost as xgb
import pandas as pd
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# 1. **Prepare Data for XGBoost**
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)
doot = xgb.DMatrix(X_oot, label=y_oot)

# 2. **Define Model Parameters**
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'learning_rate': 0.01,
    'max_depth': 2,
    'subsample': 0.3,
    'colsample_bytree': 0.3,
    'lambda': 5.5,
    'alpha': 4.8,
    'gamma': 4.5,
    'scale_pos_weight': 3.2,
    'random_state': 42
}

# 3. **Train the Model**
evals = [(dtrain, 'train'), (dtest, 'eval')]
xgb_model = xgb.train(
    params,
    dtrain,
    num_boost_round=2000,
    early_stopping_rounds=50,
    evals=evals,
    verbose_eval=10
)

# 4. **Predictions**
X_train['y_actual'] = y_train
X_train['y_pred'] = xgb_model.predict(dtrain)
X_test['y_actual'] = y_test
X_test['y_pred'] = xgb_model.predict(dtest)
X_oot['y_actual'] = y_oot
X_oot['y_pred'] = xgb_model.predict(doot)

# 5. **Decile Analysis Function**
def decile_analysis(df, prediction_col, actual_col, label_good=0, label_bad=1):
    df['Decile'] = pd.qcut(df[prediction_col], 10, duplicates='drop', labels=False) + 1
    decile_summary = df.groupby('Decile').agg(
        mean_actual=(actual_col, 'mean'),
        mean_prediction=(prediction_col, 'mean'),
        max_prediction=(prediction_col, 'max'),
        min_prediction=(prediction_col, 'min'),
        count=(actual_col, 'count'),
        label_good=(actual_col, lambda x: (x == label_good).sum()),
        label_bad=(actual_col, lambda x: (x == label_bad).sum())
    ).sort_index(ascending=False)
    
    # Calculate Metrics
    auc = roc_auc_score(df[actual_col], df[prediction_col])
    gini = 2 * auc - 1
    fpr, tpr, thresholds = roc_curve(df[actual_col], df[prediction_col])
    ks = (tpr - fpr).max()
    
    return decile_summary, auc, gini, ks

# 6. **Run Decile Analysis**
train_summary, train_auc, train_gini, train_ks = decile_analysis(X_train, 'y_pred', 'y_actual')
test_summary, test_auc, test_gini, test_ks = decile_analysis(X_test, 'y_pred', 'y_actual')
oot_summary, oot_auc, oot_gini, oot_ks = decile_analysis(X_oot, 'y_pred', 'y_actual')

# 7. **Print Results**
print(f"Train AUC: {train_auc:.2f}, Gini: {train_gini:.2f}, KS: {train_ks:.2f}")
print(f"Test AUC: {test_auc:.2f}, Gini: {test_gini:.2f}, KS: {test_ks:.2f}")
print(f"OOT AUC: {oot_auc:.2f}, Gini: {oot_gini:.2f}, KS: {oot_ks:.2f}")

# 8. **Feature Importance**
feature_importance = xgb_model.get_score(importance_type='weight')
importance_df = pd.DataFrame(list(feature_importance.items()), columns=['Feature', 'Importance'])
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# 9. **Plot Feature Importance**
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
plt.gca().invert_yaxis()
plt.show()








 
model_data['Weight_vintage'] = model_data['month_year'].apply(map_weight)
 
# Drop Null values of Target 
df = model_data.dropna(subset=['90_in_24mob'])
# take one copy for model development data
data = df.copy()
#some bank data have inf very rate but by safe replace 
data.replace([np.inf, -np.inf], np.nan, inplace=True)
 
model_df1 = data[(data['month_year']>='2021-01') & (data['month_year']<='2022-06')]
oot_df1= data[(data['month_year']>='2022-07') & (data['month_year']<='2022-09')]
# Selected Features for model build purpose
model_df = model_df1[features_list]
# Separate features (X) and target (y) for Model and OOT datasets
X_model = model_df.drop(columns=['month_year', '90_in_24mob']) # Adjust 'target' to your actual target column name
y_model = model_df['90_in_24mob']
X_oot = oot_df.drop(columns=['month_year', '90_in_24mob'])
y_oot = oot_df['90_in_24mob']
 
#Handling Null Values
for i in X_model.columns:
    X_model[i].fillna(-9999,inplace=True)
 
for i in X_oot.columns:
    X_oot[i].fillna(-9999,inplace=True)
 
### Split Model dataset into Train and Test
 
X_train, X_test, y_train, y_test = train_test_split(X_model, y_model, test_size=0.40,random_state=42)
 
#Split Training data into 80% train and 20% eval
X_train_new, X_eval, y_train_new, y_eval = train_test_split(
    X_train, y_train, test_size=0.2, random_state =42
)
print(X_train_new.shape, X_eval.shape, X_test.shape)
 
#Get vintage weight values
train_weights = X_train_new['Weight_vintage'].values
eval_weights = X_eval['Weight_vintage'].values
test_weights = X_test['Weight_vintage'].values
oot_weights = X_oot['Weight_vintage'].values
 
X_train_new = X_train_new.drop(columns=['Weight_vintage'])
X_eval = X_eval.drop(columns=['Weight_vintage'])
X_test = X_test.drop(columns=['Weight_vintage'])
X_oot = X_oot.drop(columns=['Weight_vintage'])
 
# 2. **Prepare DMatrix for XGBoost**
dtrain = xgb.DMatrix(X_train_new, label=y_train_new, weight=train_weights)
deval = xgb.DMatrix(X_eval, label=y_eval,weight=eval_weights)
dtest = xgb.DMatrix(X_test, label=y_test)
doot = xgb.DMatrix(X_oot, label=y_oot)
 
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'learning_rate': 0.01,
    'max_depth': 2,
    'subsample': 0.3,
    'colsample_bytree': 0.3,
    'lambda': 12.6,
    'alpha': 4.8,
    'gamma': 5.6,
    'scale_pos_weight': 3.8,
    'random_state': 24   
}
 
 
evals = [(dtrain, 'train'), (dtest, 'eval')]
xgb_model = xgb.train(
    params,
    dtrain,
    num_boost_round= 500,
    early_stopping_rounds= 40,
    evals=evals,
    verbose_eval=10
)
 
X_train_pred = X_train_new.copy()
X_eval_pred = X_eval.copy()
X_test_pred = X_test.copy()
X_oot_pred = X_oot.copy()
 
 
X_train_pred['y_actual'] = y_train_new
X_train_pred['y_pred'] = xgb_model.predict(dtrain)
X_train_pred['LID'] = model_df1.loc[X_train_new.index,'latest_feature_id'].values
 
X_eval_pred['y_actual'] = y_eval
X_eval_pred['y_pred'] = xgb_model.predict(deval)
X_eval_pred['LID'] = model_df1.loc[X_eval.index,'latest_feature_id'].values
 
X_test_pred['y_actual'] = y_test
X_test_pred['y_pred'] = xgb_model.predict(dtest)
X_test_pred['LID'] = model_df1.loc[X_test.index,'latest_feature_id'].values
 
X_oot_pred['y_actual'] = y_oot
X_oot_pred['y_pred'] = xgb_model.predict(doot)
X_oot_pred['LID'] = oot_df1.loc[X_oot.index,'latest_feature_id'].values
 
# 6. **Run Decile Analysis**
train_summary, train_auc, train_gini, train_ks = decile_analysis(X_train_pred, 'y_pred', 'y_actual')
eval_summary, eval_auc, eval_gini, eval_ks = decile_analysis(X_eval_pred, 'y_pred', 'y_actual')
 
test_summary, test_auc, test_gini, test_ks = decile_analysis(X_test_pred, 'y_pred', 'y_actual')
oot_summary, oot_auc, oot_gini, oot_ks = decile_analysis(X_oot_pred, 'y_pred', 'y_actual')
 
# 7. **Print Results**
print(f"Train AUC: {train_auc:.2f}, Gini: {train_gini:.2f}, KS: {train_ks:.2f}")
print(f"Eval AUC: {eval_auc:.2f}, Gini: {eval_gini:.2f}, KS: {eval_ks:.2f}")
print(f"Test AUC: {test_auc:.2f}, Gini: {test_gini:.2f}, KS: {test_ks:.2f}")
print(f"OOT AUC: {oot_auc:.2f}, Gini: {oot_gini:.2f}, KS: {oot_ks:.2f}")
 
#Save the model
xgb_model.save_model('.json')
print('Model Saved')
has context menu





